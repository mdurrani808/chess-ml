{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC320 Final Project: Chess Data Analysis\n",
    "##### *Mohammad Durrani and Pranav Shah*\n",
    "\n",
    "### Contents\n",
    "1. [Introduction](##1.-Introduction)\n",
    "2. [Required Tools](##2.-Required-Tools)\n",
    "3. [About The Data](##3.-About-The-Data)\n",
    "4. [Data Collection: Scraping](##4.-Data-Collection:-Scraping)\n",
    "5. [Data Processing: Cleaning](##5.-Data-Processing:-Cleaning)\n",
    "6. [Exploratory Data Analysis](##6.-Exploratory-Data-Analysis)\n",
    "7. [Future Work and Conclusion](##7.-Future-Work-and-Conclusion)\n",
    "8. [References and Additional Resources](##8.-References-and-Additional-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install chess\n",
    "!{sys.executable} -m pip install category-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. About The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Collection: Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "#pgnFilePath =  r\"\\\\wsl.localhost\\Ubuntu\\home\\mdurrani\\lichess_db_standard_rated_2024-03.pgn\"\n",
    "pgnFilePath = r'./data/*.pgn'\n",
    "#outputFilePath = \"./evaluations.csv\"\n",
    "totalGameCount = 0\n",
    "totalEvalGameCount = 0\n",
    "fileList = [] #glob.glob(pgnFilePath) # modify this line for the directory that has all the pgn files \n",
    "rows_list = [] # list of dictionaries for each game \n",
    "for file in fileList:\n",
    "    pgn = open(fileList[0])\n",
    "    game = chess.pgn.read_game(pgn)\n",
    "    while game != None:\n",
    "        totalGameCount += 1\n",
    "        variations = game.variations # list of either the eval or clock score\n",
    "        if(totalGameCount % 2000 == 0): # print every 2000 games\n",
    "            print(\"Total Game Count: \", totalGameCount)\n",
    "            print(\"Eval Game Count: \", totalEvalGameCount)\n",
    "        if(totalGameCount % 1000000 == 0):\n",
    "            df = pd.DataFrame(rows_list) # add everything to the dataframe \n",
    "            df.to_csv(outputFilePath)\n",
    "        if(len(variations) > 0 and 'eval' in variations[0].comment): # if this game was evaluated by a computer, add it\n",
    "            totalEvalGameCount+=1\n",
    "            h = game.headers\n",
    "                \n",
    "            # adding a dictionary is faster than appending to a dataframe \n",
    "            rows_list.append({\n",
    "                \"UTCDate\": h.get(\"UTCDate\", np.NaN),\n",
    "                \"UTCTime\": h.get(\"UTCTime\", np.NaN),\n",
    "                'WhiteElo': h.get('WhiteElo', np.NaN),\n",
    "                'BlackElo': h.get('BlackElo', np.NaN),\n",
    "                \"Opening\": h.get(\"Opening\", np.NaN),\n",
    "                \"ECO\": h.get(\"ECO\", np.NaN),\n",
    "                'Result': h.get('Result', np.NaN),\n",
    "                \"Termination\": h.get(\"Termination\", np.NaN),\n",
    "                \"Variations\": str(variations[0]) if variations else np.NaN,\n",
    "                'WhiteRatingDiff': h.get('WhiteRatingDiff', np.NaN),\n",
    "                'BlackRatingDiff': h.get(\"BlackRatingDiff\", np.NaN)\n",
    "            })\n",
    "\n",
    "        # Iterator reading next game\n",
    "        game = chess.pgn.read_game(pgn)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processing: Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ECO  Result  BlunderDifferential  MistakeDifferential  \\\n",
      "0  B09     0.0                    0                    2   \n",
      "1  C33     1.0                   -1                    0   \n",
      "2  B12     1.0                   -1                   -2   \n",
      "3  B13     1.0                    0                    1   \n",
      "4  C40     1.0                    0                   -3   \n",
      "\n",
      "   InaccuracyDifferential  TimeDifferential  EloDifferential  \n",
      "0                       1                26               97  \n",
      "1                      -2               -33             -128  \n",
      "2                       0              -495              406  \n",
      "3                       2              -527              342  \n",
      "4                      -3               121              441  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "# Read data back in from csv and ignore the first column because it just contains the indicies.\n",
    "# We only want white and black elo, opening category, result, and variations\n",
    "smallDataset = \"miniEvaluations.csv\"\n",
    "fullDataset = \"evaluations.csv\"\n",
    "df= pd.read_csv(smallDataset, usecols=[\"WhiteElo\", \"BlackElo\", \"ECO\", \"Result\", \"Variations\"])\n",
    "\n",
    "# All data will be from the perspective of white \n",
    "\n",
    "# Get mistake differential from a game\n",
    "def getMistakeDifferentials(variation):\n",
    "    # Find all evaluations\n",
    "    evalText = re.findall(r'%eval -?\\d.\\d*', variation)\n",
    "    # Truncate text and get float eval value\n",
    "    evalList = []\n",
    "    for eval in evalText:\n",
    "        evalList.append(float(eval.split(\" \")[1]))\n",
    "\n",
    "    # Find the mistake differential for white\n",
    "    evalDifference = 0\n",
    "    whiteBlunders = 0\n",
    "    blackBlunders = 0\n",
    "    whiteMistakes = 0\n",
    "    blackMistakes = 0\n",
    "    whiteInaccuracies = 0\n",
    "    blackInaccuracies = 0\n",
    "    for i in range(len(evalList)):\n",
    "        if i != 0:\n",
    "            evalDifference = evalList[i] - evalList[i-1]\n",
    "        if abs(evalDifference) > 3:\n",
    "            if i % 2 == 0:\n",
    "                whiteBlunders += 1\n",
    "            else:\n",
    "                blackBlunders += 1\n",
    "        elif abs(evalDifference) > 1:\n",
    "            if i % 2 == 0:\n",
    "                whiteMistakes += 1\n",
    "            else:\n",
    "                blackMistakes += 1\n",
    "        elif abs(evalDifference) > 0.5:\n",
    "            if i % 2 == 0:\n",
    "                whiteInaccuracies += 1\n",
    "            else:\n",
    "                blackInaccuracies += 1\n",
    "    blunderDifferential = whiteBlunders - blackBlunders\n",
    "    mistakeDifferential = whiteMistakes - blackMistakes\n",
    "    inaccuracyDifferential = whiteInaccuracies - blackInaccuracies\n",
    "    return pd.Series({\"BlunderDifferential\" : blunderDifferential, \"MistakeDifferential\" : mistakeDifferential, \n",
    "                      \"InaccuracyDifferential\" : inaccuracyDifferential})\n",
    "\n",
    "# Get time differential from a game\n",
    "def getTimeDifferential(variation):\n",
    "    # Find all clock text\n",
    "    clockText = re.findall(r'%clk \\d:\\d{2}:\\d{2}', variation)\n",
    "    # Truncate text and get clock data in seconds\n",
    "    clockList = []\n",
    "    for clock in clockText:\n",
    "        time = clock.split(\" \")[1]\n",
    "        hr = int(time.split(\":\")[0])\n",
    "        min = int(time.split(\":\")[1])\n",
    "        sec = int(time.split(\":\")[2])\n",
    "        totalSeconds = hr * 3600 + min * 60 + sec\n",
    "        clockList.append(totalSeconds)\n",
    "\n",
    "    # Get clock differential\n",
    "    whiteTimeElapsed = 0\n",
    "    blackTimeElapsed = 0\n",
    "    # If less than 2 moves, then don't worry about calculating time elapsed\n",
    "    whiteBeginClock = 0\n",
    "    if len(clockList) >= 2:\n",
    "        whiteBeginClock = clockList[0]\n",
    "        blackBeginClock = clockList[1]\n",
    "        if len(clockList) % 2 == 0:\n",
    "            whiteEndClock = clockList[-2]\n",
    "            blackEndClock = clockList[-1]\n",
    "        else:\n",
    "            whiteEndClock = clockList[-1]\n",
    "            blackEndClock = clockList[-2]\n",
    "        whiteTimeElapsed = whiteBeginClock - whiteEndClock\n",
    "        blackTimeElapsed = blackBeginClock - blackEndClock\n",
    "    \n",
    "    timeDifferential = whiteTimeElapsed - blackTimeElapsed\n",
    "    if whiteBeginClock != 0:\n",
    "        timeDifferential /= whiteBeginClock\n",
    "\n",
    "    return timeDifferential\n",
    "\n",
    "    \n",
    "# Turn string result into a number result \n",
    "def getResultForWhite(result):\n",
    "    if result == \"0-1\":\n",
    "        return 0\n",
    "    elif result == \"1-0\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.5\n",
    "    \n",
    "df[[\"BlunderDifferential\",\"MistakeDifferential\",\"InaccuracyDifferential\"]] = df[\"Variations\"].apply(getMistakeDifferentials)\n",
    "\n",
    "df[\"TimeDifferential\"] = df[\"Variations\"].apply(getTimeDifferential)\n",
    "\n",
    "df[\"EloDifferential\"] = df[\"WhiteElo\"] - df[\"BlackElo\"]\n",
    "\n",
    "df[\"Result\"] = df[\"Result\"].apply(getResultForWhite)\n",
    "\n",
    "df = df.drop([\"Variations\", \"WhiteElo\", \"BlackElo\"], axis=1)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the relationship between each of our differentials and the average result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bins =[-15,-10,-5,0,5,10,15]\n",
    "\n",
    "df[\"MistakeGroup\"] = pd.cut(df['MistakeDifferential'], bins=bins, precision=0)\n",
    "\n",
    "# Drop na values caused by values that don't fit the bins\n",
    "groups = df[\"MistakeGroup\"].dropna().unique().sort_values()\n",
    "\n",
    "x = range(len(groups))\n",
    "y = df.groupby(\"MistakeGroup\", observed=True)[\"Result\"].mean()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(x, y)\n",
    "plt.xticks(x, groups)\n",
    "plt.xlabel('Mistake Differential Group')\n",
    "plt.ylabel('Mean Result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that making less mistakes does influence winning. However, the graph does not show the straight correlation that was expected. This result may be due to mistakes varying in size: mistakes are classified as anything that changes the evaluation of the game by one or more, but regardless of the change in evaluation after 1, the mistakes are weighted the same. It seems that with a large mistake differential winning was less predictable. This is probably because the player with a mistake advantage must have made some very large mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1500,-1000,-500,0,500,1000,1500]\n",
    "df[\"EloGroup\"] = pd.cut(df['EloDifferential'], bins=bins, precision=0)\n",
    "\n",
    "# Drop na values caused by values that don't fit the bins\n",
    "groups = df[\"EloGroup\"].dropna().unique().sort_values()\n",
    "\n",
    "x = range(len(groups))\n",
    "y = df.groupby(\"EloGroup\", observed=True)[\"Result\"].mean()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(x, y)\n",
    "\n",
    "plt.xticks(x, groups)\n",
    "plt.xlabel('Elo Differential Group')\n",
    "plt.ylabel('Mean Result')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elo Differential graph shows that elo has a clear correlation towards winning. The greater the differential between the player and their opponent, the greater the chance of winning. However elo doesn't impact the game significantly, at least not within a differential range of -500 to 500. As long as players were in that elo range they were only at a less than 5% disadvantage/advantage. This is much different from the mistake graph where having the mistake advantage could increase chances of winning by more than 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-300,-240,-180,-120,-60,0,60,120,180,240,300]\n",
    "df[\"TimeGroup\"] = pd.cut(df['TimeDifferential'],bins=bins, precision= 0)\n",
    "\n",
    "# Drop na values caused by values that don't fit the bins\n",
    "groups = df[\"TimeGroup\"].dropna().unique().sort_values()\n",
    "\n",
    "x = range(len(groups))\n",
    "y = df.groupby(\"TimeGroup\", observed=True)[\"Result\"].mean()\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(x, y)\n",
    "plt.xticks(x, groups)\n",
    "plt.xlabel('Time Differential Group')\n",
    "plt.ylabel('Mean Result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the graph, it seems that the only factor that influences winning is whether the player has the time advantage, not the quantity of that advantage. This makes sense as games can be won on time, so a significant decline in results can be seen near 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter our df by openings that have at least 10000 games played, approximately 1% of the dataset. \n",
    "# We want frequently played openings because unusual openings have a large variance to their win percentage.\n",
    "value_counts = df[\"ECO\"].value_counts()\n",
    "\n",
    "filtered_values = value_counts[value_counts > len(df.index)*.01].index\n",
    "filtered_df = df[df['ECO'].isin(filtered_values)]\n",
    "\n",
    "groups = filtered_df['ECO'].unique()\n",
    "\n",
    "x = range(len(groups))\n",
    "y = filtered_df.groupby(\"ECO\", observed=True)[\"Result\"].mean()\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.bar(x, y)\n",
    "plt.xticks(x, groups)\n",
    "plt.xlabel('Openings')\n",
    "plt.ylabel('Mean Result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filtered the data to only include openings that accounted for at least 1% of games in our database to get results with a low variance. The results show that openings do have a small impact on winning. The least and most winning openings differed by about 20%, with most openings hovering close to the 50% win rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore how our features behave with one another to get a better insight into how they impact winning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Filter dataframe to remove outliers to make axes smaller\n",
    "mincnt = 300\n",
    "# If using small dataset\n",
    "if len(df.index) < 100000:\n",
    "    mincnt = 10\n",
    "\n",
    "# Filtering dataframe to remove outliers because outliers will expand the range of the graphs significantly and \n",
    "# make trends in the graph tougher to notice.\n",
    "filtered_df = df[(abs(df[\"EloDifferential\"]) < 200) & (abs(df[\"TimeDifferential\"]) < 200)]\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 20))\n",
    "\n",
    "axesArr = [(\"EloDifferential\",\"MistakeDifferential\"),(\"EloDifferential\",\"TimeDifferential\"),(\"TimeDifferential\",\"MistakeDifferential\")]\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    x, y = axesArr[i]\n",
    "    # Decided to use a hexbin as there is a signficant amount of data, so scatter plots wouldnt work well\n",
    "    ax.hexbin(filtered_df[x],filtered_df[y],gridsize=25,mincnt=mincnt, bins=\"log\")\n",
    "    # Getting a line of best fit using linear regression\n",
    "    m,b = np.polyfit(filtered_df[x],filtered_df[y],1)\n",
    "    # Plotting this line \n",
    "    ax.plot(filtered_df[x],m*filtered_df[x]+b, 'r')\n",
    "    print(f\"Slope for {x} vs {y}: {m}\")\n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs above provide the correlation between our features and give us a more nuanced look at the impact of the features on winning rates. Hexbin graphs were chosen as there is a signficant amount of data, so scatter plots wouldnt work well. We also used linear regression to draw a line of best fit for each of these graphs.\n",
    "\n",
    "The first graph between EloDifferential and MistakeDifferential shows that having a greater elo than your opponent can reduce your mistakes slightly. A close look at the hexbins shows this slight correlation. This makes sense intuitively as higher elo players are better than their opponents, and will make fewer mistakes. \n",
    "\n",
    "The second graph between EloDifferential and TimeDifferential also agrees with our past findings, having a greater elo will give a slight time advantage and therefore increase chances of winning. This graph also had a signficantly greater slope for its line of best fit than the other two, showing a much stronger correlation between these two characteristics. \n",
    "\n",
    "The third graph between TimeDifferential and MistakeDifferential, however, defies intuition. One would think that having a time disadvantage would cause a player to panic and make more mistakes, but there seems to be little to no correlation between the two variables. The slope of the line of best fit is about half that of the first graph and there is no noticable trend when looking at the hexbin graph. One explanation for this might be that games with a large time differential may end by time out, so mistakes wouldn't factor into the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Loss       0.78      0.77      0.78      3413\n",
      "         Tie       0.00      0.00      0.00         5\n",
      "         Win       0.80      0.77      0.79      3626\n",
      "\n",
      "    accuracy                           0.77      7044\n",
      "   macro avg       0.53      0.51      0.52      7044\n",
      "weighted avg       0.79      0.77      0.78      7044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def convertWinToCategory(val):\n",
    "    res = \"\"\n",
    "    match val:\n",
    "        case 1.0:\n",
    "            res = \"Win\"\n",
    "        case 0.0:\n",
    "            res = \"Loss\" \n",
    "        case 0.5:\n",
    "            res = \"Tie\"\n",
    "    return res\n",
    "train_feat_df = df.loc[:,[\"InaccuracyDifferential\", \"MistakeDifferential\", \"BlunderDifferential\", \"TimeDifferential\", \"EloDifferential\", \"ECO\", \"Result\"]]\n",
    "\n",
    "value_counts = train_feat_df[\"ECO\"].value_counts()\n",
    "\n",
    "filtered_values = value_counts[value_counts > len(train_feat_df.index)*.02].index\n",
    "train_feat_df = train_feat_df[train_feat_df['ECO'].isin(filtered_values)]\n",
    "\n",
    "# convert results to categorical values for training\n",
    "train_feat_df[\"Result\"] = train_feat_df[\"Result\"].map(convertWinToCategory)\n",
    "\n",
    "#setup binary encoder and fit onto openings\n",
    "label_encoder = ce.BinaryEncoder()\n",
    "bin_enc_df = label_encoder.fit_transform(train_feat_df[\"ECO\"])\n",
    "train_feat_df = train_feat_df.join(bin_enc_df)\n",
    "train_feat_df = train_feat_df.drop(\"ECO\", axis = 1)\n",
    "X = train_feat_df.drop(\"Result\", axis=1)\n",
    "y = train_feat_df[\"Result\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.feature_importances_\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=6, max_leaf_nodes=9, n_estimators=150)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "param_grid = { \n",
    "    'n_estimators': [25, 50, 100, 150], \n",
    "    'max_features': ['sqrt', 'log2', None], \n",
    "    'max_depth': [3, 6, 9], \n",
    "    'max_leaf_nodes': [3, 6, 9], \n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(), \n",
    "                                   param_grid) \n",
    "random_search.fit(X_train, y_train) \n",
    "print(random_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Loss       0.76      0.77      0.76      3326\n",
      "         Tie       0.00      0.00      0.00         0\n",
      "         Win       0.80      0.76      0.78      3718\n",
      "\n",
      "    accuracy                           0.76      7044\n",
      "   macro avg       0.52      0.51      0.52      7044\n",
      "weighted avg       0.78      0.76      0.77      7044\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavshah/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/pranavshah/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/pranavshah/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_random = RandomForestClassifier(max_depth=3, \n",
    "                                      max_features='sqrt', \n",
    "                                      max_leaf_nodes=6, \n",
    "                                      n_estimators=25) \n",
    "model_random.fit(X_train, y_train) \n",
    "y_pred_rand = model.predict(X_test) \n",
    "print(classification_report(y_pred_rand, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Future Work and Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. References and Additional Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
